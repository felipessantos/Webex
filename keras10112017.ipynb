{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "labels_arq = './base_treino.csv'\n",
    "data_dir = './base_treino/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(labels_arq, sep=';', header=None)\n",
    "labels.columns = ['id', 'url', '_label_']\n",
    "labels.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "labels['_label_'] = le.fit_transform(labels._label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = labels.sample(frac = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(data_dir)\n",
    "lista_arquivos = os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features = []\n",
    "for idx, home in enumerate(lista_arquivos):\n",
    "    with open(home, 'rb') as fp:\n",
    "        df_features.append((home, fp.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(df_features, )\n",
    "df_features.columns = ['url', 'conteudo']\n",
    "\n",
    "df_features = pd.merge(df_features, labels)\n",
    "df_features.drop('url', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_size = len(df_features['_label_'])\n",
    "df_features = df_features[df_features.apply(lambda x: (not 'Internal Server Error' in x.conteudo.decode('utf-8')) & (not '404' in x.conteudo.decode('utf-8')), axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RE_D = re.compile('\\d')\n",
    "def nao_tem_numero(string):\n",
    "    return not bool(RE_D.search(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def limpeza_numeros(txt):\n",
    "    exclusao = set('1234567890')\n",
    "    lista_retorno = [value for value in  txt.split(' ') if (len(set(value) & exclusao) is 0)]\n",
    "    return ' '.join(lista_retorno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limpeza_beatiful_soup(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    \n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    text = limpeza_numeros(text.replace('\\n',' ').replace('\\t', ' ').replace('.','').replace('-','').replace('/',''))\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features['conteudo'] = df_features.conteudo.apply(lambda x: limpeza_beatiful_soup(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_list = ['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'Em', 'um', 'para', 'é', 'com', \n",
    "                   'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'foi', \n",
    "                   'ao', 'ele', 'das', 'tem', 'à', 'seu', 'sua', 'ou', 'ser', 'quando', 'muito', 'há', \n",
    "                   'nos', 'já', 'está', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', \n",
    "                   'era', 'depois', 'sem', 'mesmo', 'aos', 'ter', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', \n",
    "                   'estão', 'você', 'tinha', 'foram', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', \n",
    "                   'têm', 'numa', 'pelos', 'elas', 'havia', 'seja', 'qual', 'será', 'nós', 'tenho', 'lhe', \n",
    "                   'deles', 'essas', 'esses', 'pelas', 'este', 'fosse', 'dele', 'tu', 'te', 'vocês', 'vos', \n",
    "                   'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', \n",
    "                   'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', \n",
    "                   'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', \n",
    "                   'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', \n",
    "                   'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessemos', 'estivessem', 'estiver', \n",
    "                   'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', \n",
    "                   'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', \n",
    "                   'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', \n",
    "                   'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', \n",
    "                   'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', \n",
    "                   'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', \n",
    "                   'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', \n",
    "                   'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', \n",
    "                   'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', \n",
    "                   'teria', 'teríamos', 'teriam', 'a', 'about', 'above', 'after', 'again', 'against', 'all', \n",
    "                   'am', 'an', 'and', 'any', 'are', 'arent', 'as', 'at', 'be', 'because', 'been', 'before', \n",
    "                   'being', 'below', 'between', 'both', 'but', 'by', 'cant', 'cannot', 'could', 'couldnt', \n",
    "                   'did', 'didnt', 'do', 'does', 'doesnt', 'doing', 'dont', 'down', 'during', 'each', 'few', \n",
    "                   'for', 'from', 'further', 'had', 'hadnt', 'has', 'hasnt', 'have', 'havent', 'having', 'he', \n",
    "                   'hed', 'hes', 'her', 'here', 'heres', 'hers', 'herself', 'him', 'himself', 'his', 'how', \n",
    "                   'hows', 'i', 'id', 'im', 'ive', 'if', 'in', 'into', 'is', 'isnt', 'it', 'its', 'its', \n",
    "                   'itself', 'lets', 'me', 'more', 'most', 'mustnt', 'my', 'myself', 'no', 'nor', 'not', 'of', \n",
    "                   'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', \n",
    "                   'own', 'same', 'shant', 'she', 'shed', 'shell', 'shes', 'should', 'shouldnt', 'so', 'some', \n",
    "                   'such', 'than', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \n",
    "                   'theres', 'these', 'they', 'theyd', 'theyll', 'theyre', 'theyve', 'this', 'those', 'through', \n",
    "                   'to', 'too', 'under', 'until', 'up', 'very', 'was', 'wasnt', 'we', 'wed', 'well', 'were', \n",
    "                   'weve', 'were', 'werent', 'what', 'whats', 'when', 'whens', 'where', 'wheres', 'which', 'while', \n",
    "                   'who', 'whos', 'whom', 'why', 'whys', 'with', 'wont', 'would', 'wouldnt', 'you', 'youd', 'youll', \n",
    "                   'youre', 'youve', 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer(strip_accents='ascii', min_df = 40, max_df = 0.5, stop_words = stop_words_list)\n",
    "vec_cv = cv\n",
    "df_tokens_cv = pd.DataFrame(vec_cv.fit_transform(df_features.conteudo).toarray(),\n",
    "                         index = df_features.index).rename(columns = {v:k for k,v in vec_cv.vocabulary_.items()})\n",
    "df_features_cv = pd.concat([df_features, df_tokens_cv], axis = 1).drop('conteudo', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 5\n",
    "X_cv = df_features_cv.drop('_label_', axis = 1).values\n",
    "y = df_features_cv._label_.values\n",
    "train_size = 0.8\n",
    "x_train_cv, x_test_cv = X_cv[:int(len(X_cv)*train_size)+1],X_cv[int(len(X_cv)*train_size)+1: len(X_cv)]\n",
    "y_train, y_test = y[:int(len(y)*train_size)+1],y[int(len(y)*train_size)+1: len(y)]\n",
    "num_classes = 10\n",
    "y_nn_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_nn_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_model_():\n",
    "    num_features = len(x_train_cv[0])\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(num_features/4),activation='relu',input_shape=(num_features,),name = 'layer_1'))   \n",
    "    model.add(Dropout(0.01))\n",
    "    model.add(Dense(int(num_features/8), activation='sigmoid', name = 'layer_2'))\n",
    "    model.add(Dropout(0.01))\n",
    "    model.add(Dense(int(num_features/64), activation='sigmoid', name = 'layer_3'))\n",
    "    model.add(Dropout(0.01))\n",
    "    model.add(Dense(10, activation='softmax', name = 'output_layer'), )\n",
    "    return model\n",
    "model = _get_model_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 801 samples, validate on 199 samples\n",
      "Epoch 1/5\n",
      "801/801 [==============================] - 0s - loss: 2.3302 - acc: 0.4195 - val_loss: 2.3327 - val_acc: 0.4020\n",
      "Epoch 2/5\n",
      "801/801 [==============================] - 0s - loss: 2.3038 - acc: 0.4220 - val_loss: 2.3121 - val_acc: 0.4070\n",
      "Epoch 3/5\n",
      "801/801 [==============================] - 0s - loss: 2.2816 - acc: 0.4345 - val_loss: 2.2935 - val_acc: 0.4070\n",
      "Epoch 4/5\n",
      "801/801 [==============================] - 0s - loss: 2.2576 - acc: 0.4482 - val_loss: 2.2762 - val_acc: 0.4070\n",
      "Epoch 5/5\n",
      "801/801 [==============================] - 0s - loss: 2.2392 - acc: 0.4507 - val_loss: 2.2604 - val_acc: 0.4121\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_cv, y_nn_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,   # baixo aprende pouco, alto pode causar overfitting\n",
    "                    verbose = 1,\n",
    "                    validation_data = (x_test_cv, y_nn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_cv, y_nn_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_cv = df_features_cv.drop('_label_', axis = 1).values\n",
    "x_base = X_cv[:int(len(X_cv))+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_base)\n",
    "y_pred = pd.DataFrame(data=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred['Result_Keras'] = y_pred[[0,1,2,3,4,5,6,7,8,9]].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result = pd.concat([df_features_cv.reset_index(), y_pred], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = _get_model_()\n",
    "model.save('/home/felipe/webex/modelo_sinistro.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
